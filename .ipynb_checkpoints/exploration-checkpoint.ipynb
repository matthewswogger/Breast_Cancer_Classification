{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from breast_cancer_functions import pick_best_features, how_many_features_do_we_want, Grid_Search_All\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('breast_cancer.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "df = pd.read_sql('''SELECT *\n",
    "                    FROM cancer''', conn)\n",
    "\n",
    "# this gets run when I'm done working for the session\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will use\n",
    "all_ = list(df.columns[2:])\n",
    "X = df[all_]\n",
    "X = X.assign(const=1)\n",
    "\n",
    "# make the y out of the diagnosis column, this can be used for all of the dataframes\n",
    "y = [1 if diag == 'M' else 0 for diag in df.diagnosis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running gridsearch on a bunch of different classifiers and expecting models like the SVM to perform better than little ol LogisticRegression and having it not, I decided to Scale the data around 0. This should help the SVM perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this scales the data around 0 so no one feature takes over\n",
    "X[X.columns] = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check features for how useful they are, check my functions file for more indepth explanation\n",
    "num_features_to_check = X.shape[1]\n",
    "features_ranking = pick_best_features(X, y, num_features_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how many features I should use, check my functions file for more indepth explanation\n",
    "results, features = how_many_features_do_we_want(features_ranking, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in results.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of diagnosing breast cancer false positives are more acceptable than false negatives. If we incorrectly tell someone she has breast cancer we can test again to see if we got it wrong, she's scared for a bit but we don't send her away with cancer. If we incorrectly tell someone she doesn't have breast cancer and she trully does, she walks out thinking she is in the clear while the cancer may be getting worse, not okay.\n",
    "\n",
    "With that said, looking at the results from testing LinearRegression models with different number of features it looks like it really stops improving at about the top 16 features. After that there is improvement but not much and I haven't even done a train test split or cross validation yet; I'm not modeling anything yet, just figureing out what features I want to use. Basically it's EDA without graphing anything.\n",
    "\n",
    "So I am going to take the first 13 spots in features: `features[:16]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will be working with\n",
    "X = X[features[:16]]\n",
    "\n",
    "# split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model with cross validation\n",
    "model = LogisticRegression()\n",
    "predicted = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for num in zip(np.array(y_train),predicted):\n",
    "    if num == (1,1):\n",
    "        tp += 1\n",
    "    elif num == (1,0):\n",
    "        fn += 1\n",
    "    elif num == (0,1):\n",
    "        fp += 1\n",
    "    elif num == (0,0):\n",
    "        tn += 1\n",
    "        \n",
    "tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "pre = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print 'Precision: {}'.format(precision_score(y_test, pre, average='binary'))\n",
    "print 'Recall: {}'.format(recall_score(y_test, pre, average='binary'))\n",
    "print 'F1 score: {}'.format(f1_score(y_test, pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: tp / (tp + fp)\n",
    "\n",
    "Recall: tp / (tp + fn)\n",
    "\n",
    "F1 = 2(Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "So this wasn't a fluke, these 16 features are the ones I will use moving forward. \n",
    "\n",
    "features = 'area_worst',\n",
    " 'concave_points_worst',\n",
    " 'radius_worst',\n",
    " 'radius_se',\n",
    " 'texture_worst',\n",
    " 'perimeter_worst',\n",
    " 'concave_points_mean',\n",
    " 'area_se',\n",
    " 'concavity_worst',\n",
    " 'compactness_se',\n",
    " 'smoothness_worst',\n",
    " 'area_mean',\n",
    " 'perimeter_se',\n",
    " 'compactness_mean',\n",
    " 'symmetry_worst',\n",
    " 'radius_mean'\n",
    "\n",
    "I'm going to perform grid search on every classification model I can think of and compare them tomorrow.\n",
    "\n",
    "My first run through of grid search through a bunch of different classification models will be, what's a good way to put it, 'shallow'. I will intentionally leave out some hyperparameters that can help a model perform better in the name of speed. Narrow down what model I want to use and then grid search that one with all of the hyperparameters I can think of and really dial it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "models = {'LogisticRegression':LogisticRegression(),\n",
    "          'RandomForestClassifier':RandomForestClassifier(),\n",
    "          'ExtraTreesClassifier':ExtraTreesClassifier(),\n",
    "          'AdaBoostClassifier':AdaBoostClassifier(),\n",
    "          'GradientBoostingClassifier':GradientBoostingClassifier(),\n",
    "          'LinearSVC':LinearSVC(),\n",
    "          'SVC':SVC(),\n",
    "          'BaggingClassifier':BaggingClassifier(),\n",
    "          'SGDClassifier':SGDClassifier()\n",
    "         }\n",
    "\n",
    "parameters = {'LogisticRegression':{'C':[0.01, 0.1, 1.0, 10.0, 100.0,1000.0]},\n",
    "              'RandomForestClassifier':{'n_estimators':[16,32,64,128,256]},\n",
    "              'ExtraTreesClassifier':{'n_estimators':[16,32,64,128,256]},\n",
    "              'AdaBoostClassifier':{'n_estimators':[16,32,64,128, 150],'learning_rate':[0.5,0.8,1.0,1.2,1.5]},\n",
    "              'GradientBoostingClassifier':{'n_estimators':[64,128,150,200],'learning_rate':[0.01,0.08,0.1,0.2,0.4]},\n",
    "              'LinearSVC':{'C':[0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0,1000.0,10000.0]},\n",
    "              'SVC':{'C':[0.5, 1.0, 10.0, 100.0,1000.0,10000.0]},\n",
    "              'BaggingClassifier':{'n_estimators':[5, 10, 15, 20, 25, 30, 100, 150, 200]},\n",
    "              'SGDClassifier':{'alpha':[0.000001, 0.00001, 0.0001, 0.001, 0.01]}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_grid_search = Grid_Search_All(models,parameters)\n",
    "first_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_grid_search.score_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's looking like I'm going with LinearSVC(C=0.05)\n",
    "\n",
    "### After running grid search on all of the classifiers above LinearSVC was the winner but what the penalty parameter 'C' should be still needs to be grid searched more.\n",
    "\n",
    "#### A few notes, this is a small data set, less than 600. Because of this train test splits of the data make an obvious difference, there can be a large variance between different runs of train test splits, LinearSVC was not always the clear winner, it was the winner on average. In fact if I had gone with the first run though of grid search I would have been using LogisticRegression.\n",
    "\n",
    "#### Because of this I need to think about how I want to implement my final model, as of right now I think it will be LinearSVC and then I will run a bunch of different random train test splits and then average the results...I'll keep thinking on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "models2 = {'LinearSVC':LinearSVC()}\n",
    "\n",
    "parameters2 = {'LinearSVC':{'C':[0.01]\n",
    "                           }\n",
    "              }\n",
    "\n",
    "# parameters2 = {'LinearSVC':{'C':[0.01, 0.05, 0.1],\n",
    "#                             'tol':[0.00001, 0.0001, 0.001, 0.01],\n",
    "#                             'max_iter':[500, 750, 1000, 1250, 1500],\n",
    "#                             'loss':['hinge','squared_hinge']\n",
    "#                            }\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "second_grid_search = Grid_Search_All(models2,parameters2)\n",
    "second_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "second_grid_search.score_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So I started grid search for LinearSVC with a bunch of hyperparameters and was able to narrow it down to only needing one, 'C' = 0.01 gives the most consistently high results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting final modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the X I will be working with\n",
    "X = X[features[:16]]\n",
    "\n",
    "# split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(C=0.01).fit(X_train,y_train)\n",
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, I'm most concerned with False Negatives and Recall because Recall is about the false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print 'Precision: {}'.format(precision_score(y_test, predict, average='binary'))\n",
    "print 'Recall: {}'.format(recall_score(y_test, predict, average='binary'))\n",
    "print 'F1 score: {}'.format(f1_score(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for num in zip(np.array(y_test),predict):\n",
    "    if num == (1,1):\n",
    "        tp += 1\n",
    "    elif num == (1,0):\n",
    "        fn += 1\n",
    "    elif num == (0,1):\n",
    "        fp += 1\n",
    "    elif num == (0,0):\n",
    "        tn += 1\n",
    "        \n",
    "print (tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So...okay cool this works well. But I want to set my threshold at a different spot so that I skew the results away from False Negatives, only one problem, LinearSVC doesn't have a predict_proba method like LogisticRegression does, it only predicts the class, now what?\n",
    "\n",
    "### Turns out that if you take a look at the code for sklearn on GitHub LinearSVC's predict method uses a self.decision_function(X) that it uses to make the class prediction. It is simply an array that the predict method uses to determine the classes, it simply assigns a 1 to the index if the number is greater than 0 and a 0 for everthing else.\n",
    "\n",
    "### So all I have to do is manually change the threshold from a 0 to a number less than zero and the False Negative rate should fall...Neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = model.decision_function(X_test)\n",
    "indices = (scores > -0.1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Precision: {}'.format(precision_score(y_test, indices, average='binary'))\n",
    "print 'Recall: {}'.format(recall_score(y_test, indices, average='binary'))\n",
    "print 'F1 score: {}'.format(f1_score(y_test, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for num in zip(np.array(y_test),indices):\n",
    "    if num == (1,1):\n",
    "        tp += 1\n",
    "    elif num == (1,0):\n",
    "        fn += 1\n",
    "    elif num == (0,1):\n",
    "        fp += 1\n",
    "    elif num == (0,0):\n",
    "        tn += 1\n",
    "        \n",
    "print (tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another grid search is in order to find the correct threshold for my tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "X = X[features[:16]]\n",
    "\n",
    "\n",
    "precision, recall, f1 = [],[],[]\n",
    "for threshold in np.arange(0,2,0.01)*-1:\n",
    "    pre, re, f = [],[],[]\n",
    "    for _ in xrange(100):\n",
    "        # split into training and testing\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "        # model\n",
    "        model = LinearSVC(C=0.01).fit(X_train,y_train)\n",
    "        scores = model.decision_function(X_test)\n",
    "        prediction = (scores > threshold).astype(np.int)\n",
    "\n",
    "        pre.append(precision_score(y_test, prediction, average='binary'))\n",
    "        re.append(recall_score(y_test, prediction, average='binary'))\n",
    "        f.append(f1_score(y_test, prediction))\n",
    "        \n",
    "    precision.append(np.array(pre).mean())\n",
    "    recall.append(np.array(re).mean())\n",
    "    f1.append(np.array(f).mean())\n",
    "    \n",
    "d = {'precision': precision, 'recall': recall, 'f1_score': f1}\n",
    "results_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(0,2,0.01), results_df.f1_score, label='F1 Score')\n",
    "plt.plot(np.arange(0,2,0.01), results_df.precision, label='Precision')\n",
    "plt.plot(np.arange(0,2,0.01), results_df.recall, label='Recall')\n",
    "plt.legend()\n",
    "plt.xlim(0,0.6)\n",
    "plt.ylim(0.8,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So -0.13 is the threshold I'm going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "X = X[features[:16]]\n",
    "\n",
    "TP, TN, FP, FN = [],[],[],[]\n",
    "precision, recall, f1 = [],[],[]\n",
    "for _ in xrange(100):\n",
    "    # split into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "    # model\n",
    "    model = LinearSVC(C=0.01).fit(X_train,y_train)\n",
    "    scores = model.decision_function(X_test)\n",
    "    prediction = (scores > -0.13).astype(np.int)\n",
    "    \n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for num in zip(y_test, prediction):\n",
    "        if num == (1,1):\n",
    "            tp += 1\n",
    "        elif num == (1,0):\n",
    "            fn += 1\n",
    "        elif num == (0,1):\n",
    "            fp += 1\n",
    "        elif num == (0,0):\n",
    "            tn += 1\n",
    "\n",
    "    TP.append(tp)\n",
    "    TN.append(tn)\n",
    "    FP.append(fp)\n",
    "    FN.append(fn)\n",
    "    precision.append(precision_score(y_test, prediction, average='binary'))\n",
    "    recall.append(recall_score(y_test, prediction, average='binary'))\n",
    "    f1.append(f1_score(y_test, prediction))\n",
    "\n",
    "d = {'true_positive':TP,'true_negative':TN,'false_positive':FP,'false_negative':FN,\n",
    "     'precision':precision,'recall':recall,'f1_score':f1}\n",
    "outcome_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print '   Number of test cases: {}'.format(len(y_test))\n",
    "print 'Average False Positives: {}'.format(outcome_df.false_positive.mean())\n",
    "print 'Average False Negatives: {}'.format(outcome_df.false_negative.mean())\n",
    "print 'Maximum False Negatives: {}'.format(outcome_df.false_negative.max())\n",
    "print '      Average Precision: {}'.format(outcome_df.precision.mean())\n",
    "print '         Average Recall: {}'.format(outcome_df.recall.mean())\n",
    "print '      Average F-1 Score: {}'.format(outcome_df.f1_score.mean())\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(outcome_df.true_positive, label='True Positive')\n",
    "plt.plot(outcome_df.true_negative, label='True Negative')\n",
    "plt.plot(outcome_df.false_positive, label='False Positive')\n",
    "plt.plot(outcome_df.false_negative, label='False Negative')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features for this data set are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "### The false negative rate of fine needle aspirations of lumps that can be felt is about 2-4%.\n",
    "\n",
    "### My model has a false negative rate of about 1.1%.\n",
    "\n",
    "# Now all that's left to do if I was going to put this into production would be to train the model on the entire data set and then have at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
