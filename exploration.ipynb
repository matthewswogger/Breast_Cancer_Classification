{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from breast_cancer_functions import pick_best_features, how_many_features_do_we_want\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('breast_cancer.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "df = pd.read_sql('''SELECT *\n",
    "                    FROM cancer''', conn)\n",
    "\n",
    "# this gets run when I'm done working for the session\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will use\n",
    "all_ = list(df.columns[2:])\n",
    "X = df[all_]\n",
    "X = X.assign(const=1)\n",
    "\n",
    "# make the y out of the diagnosis column, this can be used for all of the dataframes\n",
    "y = [1 if diag == 'M' else 0 for diag in df.diagnosis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check features for how useful they are, check my functions file for more indepth explanation\n",
    "num_features_to_check = X.shape[1]\n",
    "features_ranking = pick_best_features(X, y, num_features_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how many features I should use, check my functions file for more indepth explanation\n",
    "results, features = how_many_features_do_we_want(features_ranking, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in results.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of diagnosing breast cancer false positives are more acceptable than false negatives. If we incorrectly tell someone she has breast cancer we can test again to see if we got it wrong, she's scared for a bit but we don't send her away with cancer. If we incorrectly tell someone she doesn't have breast cancer and she trully does, she walks out thinking she is in the clear while the cancer may be getting worse, not okay.\n",
    "\n",
    "With that said, looking at the results from testing LinearRegression models with different number of features it looks like it really stops improving at about the top 13 features. After that there is improvement but not much and I haven't even done a train test split or cross validation yet; I'm not modeling anything yet, just figureing out what features I want to use. Basically it's EDA without graphing anything.\n",
    "\n",
    "So I am going to take the first 13 spots in features: `features[:13]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will be working with\n",
    "X = X[features[:13]]\n",
    "\n",
    "# split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model with cross validation\n",
    "model = LogisticRegression()\n",
    "predicted = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for num in zip(np.array(y_train),predicted):\n",
    "    if num == (1,1):\n",
    "        tp += 1\n",
    "    elif num == (1,0):\n",
    "        fn += 1\n",
    "    elif num == (0,1):\n",
    "        fp += 1\n",
    "    elif num == (0,0):\n",
    "        tn += 1\n",
    "        \n",
    "tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this wasn't a fluke, these 13 features are the ones I will use moving forward. \n",
    "\n",
    "features = 'concavity_worst',\n",
    "   'concavity_mean',\n",
    "   'const',\n",
    "   'radius_worst',\n",
    "   'radius_mean',\n",
    "   'compactness_worst',\n",
    "   'concave_points_worst',\n",
    "   'concave_points_mean',\n",
    "   'perimeter_se',\n",
    "   'perimeter_worst',\n",
    "   'area_se',\n",
    "   'texture_se',\n",
    "   'texture_worst'\n",
    "\n",
    "I'm going to perform grid search on every classification model I can think of and compare them tomorrow.\n",
    "\n",
    "My first run through of grid search through a bunch of different classification models will be, what's a good way to put it, 'shallow'. I will intentionally leave out some hyperparameters that can help a model perform better in the name of speed. Narrow down what model I want to use and then grid search that one with all of the hyperparameters I can think of and really dial it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "models = {'LogisticRegression':LogisticRegression(),\n",
    "          'RandomForestClassifier':RandomForestClassifier(),\n",
    "          'ExtraTreesClassifier':ExtraTreesClassifier(),\n",
    "          'AdaBoostClassifier':AdaBoostClassifier(),\n",
    "          'GradientBoostingClassifier':GradientBoostingClassifier(),\n",
    "          'LinearSVC':LinearSVC(),\n",
    "          'SVC':SVC()\n",
    "         }\n",
    "\n",
    "parameters = {'LogisticRegression':{'C':[0.01, 0.1, 1.0, 10.0]},\n",
    "              'RandomForestClassifier':{'n_estimators':[16,32,64,128]},\n",
    "              'ExtraTreesClassifier':{'n_estimators':[16,32,64,128]},\n",
    "              'AdaBoostClassifier':{'n_estimators':[16,32,64,128],'learning_rate':[0.8,1.0,1.2]},\n",
    "              'GradientBoostingClassifier':{'n_estimators':[64,128,150],'learning_rate':[0.08,0.1,0.2]},\n",
    "              'LinearSVC':{'C':[0.5, 1.0, 10.0, 100.0]},\n",
    "              'SVC':{'C':[0.5, 1.0, 10.0, 100.0]}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "class Grid_Search_All:\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        \n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.gridsearches = {}\n",
    "        \n",
    "    def fit(self, X, y, cv=5, pre_dispatch=4, refit=False):\n",
    "        '''\n",
    "        Fits all of the models with all of the parameter options\n",
    "        using cross validation.\n",
    "        \n",
    "        cv = crossvalidation, default is 5\n",
    "        pre_dispatch = number of jobs run in parallel, default is 4 because\n",
    "                       my computer has 4 cores\n",
    "        refit = whether or not it will fit all data to best model from\n",
    "                crossvalidation, default is False because I don't need\n",
    "                it so it would waste time\n",
    "        '''\n",
    "        for model_name in self.keys:\n",
    "            print \"Running GridSearchCV for {}'s.\".format(model_name)\n",
    "            model = self.models[model_name]\n",
    "            par = self.params[model_name]\n",
    "            \n",
    "            grid_search = GridSearchCV(model, par, cv=cv, pre_dispatch=pre_dispatch, refit=refit)\n",
    "            grid_search.fit(X,y)\n",
    "            \n",
    "            self.gridsearches[model_name] = grid_search\n",
    "            \n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        '''\n",
    "        This builds and prints a pandas dataframe of the summary of all the\n",
    "        different fits of the models and orders them by best performing\n",
    "        in a category that you tell it to.\n",
    "        '''\n",
    "        def row(key, scores, params):\n",
    "            d = {'estimators': key,\n",
    "                 'min_score': np.min(scores),\n",
    "                 'max_score': np.max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'std_score': np.std(scores)\n",
    "                }\n",
    "            return pd.Series(dict(params.items() + d.items()))\n",
    "        \n",
    "        rows = []\n",
    "        for k in self.keys:\n",
    "            for gsc in self.gridsearches[k].grid_scores_:\n",
    "                rows.append(row(k, gsc.cv_validation_scores, gsc.parameters))\n",
    "                \n",
    "        df = pd.concat(rows, axis=1).T.sort([sort_by], ascending=False)\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_grid_search = Grid_Search_All(models,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_grid_search.score_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
