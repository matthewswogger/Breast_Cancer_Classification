{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from breast_cancer_functions import pick_best_features, how_many_features_do_we_want, Grid_Search_All\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('breast_cancer.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "df = pd.read_sql('''SELECT *\n",
    "                    FROM cancer''', conn)\n",
    "\n",
    "# this gets run when I'm done working for the session\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will use\n",
    "all_ = list(df.columns[2:])\n",
    "X = df[all_]\n",
    "X = X.assign(const=1)\n",
    "\n",
    "# make the y out of the diagnosis column, this can be used for all of the dataframes\n",
    "y = [1 if diag == 'M' else 0 for diag in df.diagnosis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running gridsearch on a bunch of different classifiers and expecting models like the SVM to perform better than little ol LogisticRegression and having it not, I decided to Scale the data around 0. This should help the SVM perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this scales the data around 0 so no one feature takes over\n",
    "X[X.columns] = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check features for how useful they are, check my functions file for more indepth explanation\n",
    "num_features_to_check = X.shape[1]\n",
    "features_ranking = pick_best_features(X, y, num_features_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how many features I should use, check my functions file for more indepth explanation\n",
    "results, features = how_many_features_do_we_want(features_ranking, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for key, value in results.iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of diagnosing breast cancer false positives are more acceptable than false negatives. If we incorrectly tell someone she has breast cancer we can test again to see if we got it wrong, she's scared for a bit but we don't send her away with cancer. If we incorrectly tell someone she doesn't have breast cancer and she trully does, she walks out thinking she is in the clear while the cancer may be getting worse, not okay.\n",
    "\n",
    "With that said, looking at the results from testing LinearRegression models with different number of features it looks like it really stops improving at about the top 16 features. After that there is improvement but not much and I haven't even done a train test split or cross validation yet; I'm not modeling anything yet, just figureing out what features I want to use. Basically it's EDA without graphing anything.\n",
    "\n",
    "So I am going to take the first 13 spots in features: `features[:16]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the X I will be working with\n",
    "X = X[features[:16]]\n",
    "\n",
    "# split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model with cross validation\n",
    "model = LogisticRegression()\n",
    "predicted = cross_val_predict(model, X_train, y_train, cv=5)\n",
    "\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for num in zip(np.array(y_train),predicted):\n",
    "    if num == (1,1):\n",
    "        tp += 1\n",
    "    elif num == (1,0):\n",
    "        fn += 1\n",
    "    elif num == (0,1):\n",
    "        fp += 1\n",
    "    elif num == (0,0):\n",
    "        tn += 1\n",
    "        \n",
    "tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "pre = model.predict(X_test)\n",
    "\n",
    "# tp, tn, fp, fn = 0, 0, 0, 0\n",
    "# for num in zip(np.array(y_test),pre):\n",
    "#     if num == (1,1):\n",
    "#         tp += 1\n",
    "#     elif num == (1,0):\n",
    "#         fn += 1\n",
    "#     elif num == (0,1):\n",
    "#         fp += 1\n",
    "#     elif num == (0,0):\n",
    "#         tn += 1\n",
    "        \n",
    "# print (tp, tn, fp, fn)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print 'Precision: {}'.format(precision_score(y_test, pre, average='binary'))\n",
    "print 'Recall: {}'.format(recall_score(y_test, pre, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: tp / (tp + fp)\n",
    "\n",
    "Recall: tp / (tp + fn)\n",
    "\n",
    "So this wasn't a fluke, these 16 features are the ones I will use moving forward. \n",
    "\n",
    "features = 'area_worst',\n",
    " 'concave_points_worst',\n",
    " 'radius_worst',\n",
    " 'radius_se',\n",
    " 'texture_worst',\n",
    " 'perimeter_worst',\n",
    " 'concave_points_mean',\n",
    " 'area_se',\n",
    " 'concavity_worst',\n",
    " 'compactness_se',\n",
    " 'smoothness_worst',\n",
    " 'area_mean',\n",
    " 'perimeter_se',\n",
    " 'compactness_mean',\n",
    " 'symmetry_worst',\n",
    " 'radius_mean'\n",
    "\n",
    "I'm going to perform grid search on every classification model I can think of and compare them tomorrow.\n",
    "\n",
    "My first run through of grid search through a bunch of different classification models will be, what's a good way to put it, 'shallow'. I will intentionally leave out some hyperparameters that can help a model perform better in the name of speed. Narrow down what model I want to use and then grid search that one with all of the hyperparameters I can think of and really dial it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "models = {'LogisticRegression':LogisticRegression(),\n",
    "          'RandomForestClassifier':RandomForestClassifier(),\n",
    "          'ExtraTreesClassifier':ExtraTreesClassifier(),\n",
    "          'AdaBoostClassifier':AdaBoostClassifier(),\n",
    "          'GradientBoostingClassifier':GradientBoostingClassifier(),\n",
    "          'LinearSVC':LinearSVC(),\n",
    "          'SVC':SVC()\n",
    "         }\n",
    "\n",
    "parameters = {'LogisticRegression':{'C':[0.01, 0.1, 1.0, 10.0, 100.0,1000.0]},\n",
    "              'RandomForestClassifier':{'n_estimators':[16,32,64,128,256]},\n",
    "              'ExtraTreesClassifier':{'n_estimators':[16,32,64,128,256]},\n",
    "              'AdaBoostClassifier':{'n_estimators':[16,32,64,128, 150],'learning_rate':[0.5,0.8,1.0,1.2,1.5]},\n",
    "              'GradientBoostingClassifier':{'n_estimators':[64,128,150,200],'learning_rate':[0.01,0.08,0.1,0.2,0.4]},\n",
    "              'LinearSVC':{'C':[0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0,1000.0,10000.0]},\n",
    "              'SVC':{'C':[0.5, 1.0, 10.0, 100.0,1000.0,10000.0]}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_grid_search = Grid_Search_All(models,parameters)\n",
    "first_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "first_grid_search.score_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's looking like I'm going with LinearSVC(C=0.05)\n",
    "\n",
    "# But I have two more models I need to grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
